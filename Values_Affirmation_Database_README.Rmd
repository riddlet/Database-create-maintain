---
title: "Values Affirmation Database README"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document: default
date: '`r format(Sys.time(), "%d %B, %Y")`'
---
----------------- 

#Introduction  

This document describes the database compiled as part of the NSF and ROADS sponsored research for the investigation of the values affirmation essays using natural language processing methodology.

The database consists of four files:

1. **demog** - This file features the demographic characteristics of the participants in the studies. As indicated by the number string in the filename, it was last updated on 9/11/2015. Similar dates are appended to the other files.
2. **essays** - This file includes the essays and their corresponding attributes.
3. **grades** - This file includes grades and other academic outcome measures for participants in the studies
4. **prompts** - This file includes characteristics of the prompts for each of the possible interventions.

Each file's name indicates the content of the data, plus the date on which the file was generated. At the time of this writing, the files have the names `demog9.11.15.csv`, `essays9.30.15.csv`, `grades10.5.15.csv`, and `prompts10.8.15.csv`.

#File format and structure
In order to make these files usable across a variety of software types and system environments, they have been stored as basic csv files. However, using commas to separate the field values in the essays and prompts file will obviously lead to problems, because the content of the fields occasionally contain commas. For that reason, these files are stored as pipe-separated values (a pipe is a |). It's possible to see the difference in structure by opening the files using a basic text-editor (textEdit on mac, notepad, sublimetext, textwrangler, etc)

The format of each file follows the general principles of [relational databases](https://en.wikipedia.org/wiki/Relational_model). Each file is stored in what is often referred to as a 'long' format, and each file features a specific type of observational unit (i.e. essays, participants, outcomes, or prompts). Within each file, these observational units are organized by the variables, which form columns, and the observations, which form rows. 

The variables for each file are listed below.

##Essays
```{r, echo=FALSE}
df.essays <- read.csv('../Data/3 CSV Files/essays11.02.15.csv', sep='|', quote="")
df.essays <- df.essays[which(df.essays$Essay != ''),]
df.essays <- df.essays[which(df.essays$Essay != 'N/A'),]
names(df.essays)
```
##Participants
```{r, echo=F}
df.demographics <- read.csv('../Data/3 CSV Files/demog9.11.15.csv')
names(df.demographics)[-6:-8]
```
##Outcomes
```{r, echo=F}
df.outcomes <- read.csv('../Data/3 CSV Files/grades10.5.15.csv')
df.outcomes <- df.outcomes[which(df.outcomes$Grade != ''),]
df.outcomes <- df.outcomes[,-1]
names(df.outcomes)
```
##Prompts
```{r, echo=F, message=FALSE}
df.prompts <- read.csv('../Data/3 CSV Files/prompts10.26.15.csv', sep='|', quote="")
df.prompts <- df.prompts[,-1]
names(df.prompts)
```

Note that this file structure makes it very clear what information is contained where. Very little explication of the content of each file is needed here.

All files with the exception of the prompts share a common *key* of the participant ID number. This allows the analyst to join together data from each file using basic join operations. Where studies had duplicate ID numbers, we appended a `.x` to the end of the ID, where x is an integer, incremented each time we encountered a file that had ID numbers that duplicated ID numbers already incorporated into the database.

The prompts file, since the unit of measure is the intervention and not the individual, does not have a specific participant ID associated with each row. Instead, the information from this file can be joined to the others using one of the multiple keys it has in common with the other files. For instance, if the analyst were interested in joining the prompts to the Essays table, one could join by the combination of `Intervention_number`, `Intervention_date`, and `Condition`, which would correctly join the information in the two, yielding a new table that contained the prompt information for each essay oin the Essays file.

Furthermore, these data are stored in what is sometimes referred to as a 'long' format. This means that there are a minimal number of columns, and each table features measurements on a single element (i.e. essays, participants, outcomes, or prompts). The rows are the individual observations


#File contents
I now use these files to illustrate some basic properties about the database. We will investigate these files one-by-one. I will also go in depth for the Windsor data in some areas.

##Essays

- **Total Number of essays**
```{r, echo=F, message=FALSE}
length(df.essays$Essay)
```

- **Number of essays per study**
```{r, echo=F, message=FALSE}
table(df.essays$Study)
```

- **Number of essays per Cohort - Windsor data only**
```{r, echo=F, message=FALSE}
library(dplyr)
temp <- filter(df.essays, Study == 'Connecticut')
temp$Cohort <- droplevels(temp$Cohort)
table(temp$Cohort)
```

- **Number of essays per interventions per Cohort - Windsor data only**
```{r, echo=F, message=FALSE}
table(temp$Intervention_number, temp$Cohort)
```

- **Number of essays per intervention date per Cohort - Windsor data only**
```{r, echo=F, message=FALSE}
temp$Intervention_Date <- droplevels(temp$Intervention_Date)
temp$Intervention_Date <- as.Date(as.character(temp$Intervention_Date), format="%m/%d/%Y")
temp <- arrange(temp, Intervention_Date)
table(temp$Intervention_Date, temp$Cohort)
```

### Annotations
We have manually annotated essays from the connecticut cohorts for grammatical and spelling errors. After several false starts, we settled on a scheme in which research assistants attempted to convert all writing into sentences that were grammatically well-formed. They accomplished this by indicating through three types of edits - additions, deletions, and substitutions. For each essay, they marked the spot of the edit through the use of symbols for each of the edit type. Additions were marked with `$` symbols, deletions with `^` symbols, and substitutions with `@` symbols, In addition, for additions and substitutions, they provided the new or replacement text. 

For instance, here is an entry that required two substitutions:

```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'Essay', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$Essay[1]))
```

In the `annotated` column of the same table, we can find where the changes were marked, as well as what the 'fix' is.

```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$annotated[1]))
```

Finally, the `corrected` column displays the essay with the suggested edits implemented.

```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$corrected[1]))
```

An example of an essay that includes deletions and substitutions:

```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'Essay', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$Essay[3]))
```
```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'annotated', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$annotated[3]))
```
```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'corrected', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$corrected[3]))

```

And an example essay with all three edit types:

```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'Essay', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$Essay[4]))
```
```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'annotated', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$annotated[4]))
```
```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'corrected', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$corrected[4]))

```

Typically, the addition, deletion, or substitution referred to only the previous word. However, on occasion it was necessary to perform an edit on a group of words. In these instances, the group of words which the edit refers to are grouped by curly brackets (i.e. `{}`). Here is an example:

```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'Essay', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$Essay[5]))
```
```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'annotated', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$annotated[5]))
```
```{r, echo=F, message=FALSE, results='asis'}
writeLines(paste('**', 'corrected', '**\n', sep=''))
```
```{r, echo=F, message=FALSE}
writeLines(strwrap(temp$corrected[5]))

```

It should be apparent from even these few examples that the edits are not always perfect. However, it is also equally apparent that, on the whole, they dramatically improve the grammar and spelling for each essay. At this time, we have performed annotations for the windsor data only. We have nearly finished with this task. 

- **Proportion of essays annotated for correction - Windsor data only**
```{r, echo=F, message=FALSE}
missed <- table(temp$annotated=='')[1] + 
  table(temp$annotated=='N/A')[1] +  
  table(is.na(temp$annotated))[2]

done <- length(temp$Essay) - missed
done/length(temp$Essay)
```

##Demographics

- **Number of participants**
```{r, echo=F, message=FALSE}
length(df.demographics$ID)
```

- **Number of participants per study**
```{r, echo=F, message=FALSE}
table(df.demographics$Study)
```

- **Breakdown of participant race by study**
```{r, echo=F, message=FALSE}
df.demographics$Ethnicity[which(df.demographics$Ethnicity == '#N/A')] <- NA
df.demographics$Ethnicity[which(df.demographics$Ethnicity == '')] <- NA
df.demographics$Ethnicity[which(df.demographics$Ethnicity == 'Other')] <- 'Other/Mixed'
df.demographics$Ethnicity <- droplevels(df.demographics$Ethnicity)
table(df.demographics$Ethnicity, df.demographics$Study)
```

- **Breakdown of participant gender by study**
```{r, echo=F, message=FALSE}
df.demographics$Gender[which(df.demographics$Gender == '#NULL!')] <- NA
df.demographics$Gender[which(df.demographics$Gender == '')] <- NA
df.demographics$Gender <- droplevels(df.demographics$Gender)
table(df.demographics$Study, df.demographics$Gender)
```

##Outcomes

- **Total number of outcome observations**
```{r, echo=F, message=FALSE}
length(df.outcomes$ID)
```

- **Number of outcome type per study**
```{r, echo=F, message=FALSE}
df.outcomes$Study <- droplevels(df.outcomes$Study)
table(df.outcomes$Grade_type, df.outcomes$Study)
```

- **Outcomes by date - Windsor data only**
```{r, echo=F, message=FALSE}
temp <- filter(df.outcomes, Study == 'Connecticut')
temp$est_Grade_date <- droplevels(temp$est_Grade_date)
levels(temp$est_Grade_date)[17:22] <- c('2/1/04', '2/1/05', '2/1/06', '6/30/04',
                                        '6/30/05', '6/30/06')
temp$est_Grade_date <- as.Date(as.character(temp$est_Grade_date), format="%m/%d/%y")
temp <- arrange(temp, est_Grade_date)
temp$Grade_type <- droplevels(temp$Grade_type)
table(temp$est_Grade_date, temp$Grade_type)
```

##Prompts
This file is still being created. The windsor data is finished, so we will limit our description to these data

- **Number of unique values combinations**
```{r, echo=F, message=FALSE}
temp <- filter(df.prompts, Study == 'Connecticut')
temp <- temp[which(temp$Values!=''),]
temp$Values <- droplevels(temp$Values)
length(unique(temp$Values))
```

- **Number of different prompts**
```{r, echo=F, message=FALSE}
temp <- filter(df.prompts, Study == 'Connecticut')
temp <- temp[which(temp$Essay.Prompt!=''),]
temp$Essay.Prompt <- droplevels(temp$Essay.Prompt)
num.prompts <- length(unique(temp$Essay.Prompt))
num.prompts
```

- **Prompt type per Cohort**
```{r, echo=F, message=FALSE, warning=F}
temp$Condition <- droplevels(temp$Condition)
levels(temp$Condition) <- c(rep('Treat', 2), rep('Control', 8), 
                            rep('Treat', 7))
C <- temp[which(temp$Condition == 'Control'),]
Tr <- temp[which(temp$Condition == 'Treat'),]
C$prompt.factor <- factor(droplevels(C$Essay.Prompt), 
                          labels=paste('C', 1:19, sep='-'))
Tr$prompt.factor <- factor(droplevels(Tr$Essay.Prompt),
                           labels=paste('T', 1:20, sep='-'))
temp <- rbind(C, Tr)
C <- reshape(C[,c(1,5,8)], timevar = "Cohort", idvar = "Intervention_number",
             direction = "wide")
Tr <- reshape(Tr[,c(1,5,8)], timevar = "Cohort", idvar = "Intervention_number",
             direction = "wide")

names(C)[2:9] <- paste('Cohort', 1:8) 
names(Tr)[2:9] <- paste('Cohort', 1:8) 

C
Tr

```

The reader may notice that the treatments for Cohorts 7 and 8 do not entirely match those listed below. For Cohort 7, Intervention 2 and Cohort 8, Intervention 1, subjects were randomly assigned to two different types of prompts - some were about politicians and others were about friends and family.

```{r, echo=F, message=FALSE, results='asis'}
  temp <- temp[order(temp$prompt.factor, temp$Cohort, temp$Intervention_number),
               c('Intervention_number', 'Cohort', 'Essay.Prompt', 
                 'prompt.factor')]
  temp <- unique(temp)
  
  writeLines(paste('###', temp$prompt.factor[1], '\n', sep=''))
  writeLines(strwrap(temp$Essay.Prompt[1]))
  writeLines('\n')
  writeLines(paste(temp$Cohort[1],'\n', sep=''))
  writeLines(strwrap('- Intervention Number(s): '))
  writeLines(strwrap(temp$Intervention_number[1]))
  
for(i in 2:length(temp$Essay.Prompt)){
  if(temp$Essay.Prompt[i-1] != temp$Essay.Prompt[i]){
    writeLines(paste('\n', '###', temp$prompt.factor[i], sep=''))
    writeLines(strwrap('\n'))
    writeLines(strwrap(temp$Essay.Prompt[i]))
    writeLines('\n')
    writeLines(strwrap('\n'))
    writeLines(paste(temp$Cohort[i], '\n', sep=''))
    writeLines(strwrap('- Intervention Number(s): '))
    writeLines(strwrap(temp$Intervention_number[i]))
    }
  else{
    if(temp$Cohort[i] != temp$Cohort[i-1]){
        writeLines(strwrap('\n'))
        writeLines(paste(temp$Cohort[i], '\n', sep=''))
        writeLines(strwrap('- Intervention Number(s): '))
        writeLines(strwrap(temp$Intervention_number[i]))
    }
    if(temp$Cohort[i] == temp$Cohort[i-1]){
      writeLines(paste(', ', temp$Intervention_number[i], sep=''))
    }
  }

}

#  writeLines((paste('###', 'Prompt' temp$prompt.factor[1])))
#  WriteLines(paste(Cohort[1], Intervention_number[1]))
#  if(temp2$Cohort[i] != temp2$Cohort[i-1]){
#    writeLines(paste('###', temp2$Cohort[i], '  '))
    #writeLines('####################\n')
#  }
 # if(temp2$Intervention_number[i] != temp2$Intervention_number[i-1]){
#    writeLines('\n-----\n')
#    writeLines(paste('####', 'Intervention Number ', temp2$Intervention_number[i], '  '))
#  }
#  writeLines(paste('######', 'Condition', temp2$Condition[i], '  '))
#  
#  writeLines(strwrap(temp2$Essay.Prompt[i]))
#  writeLines('\n')
#}

```

#Remaining tasks and Residual concerns
## CO/CA Latino Study
The other dataset for which we have extensive longitudinal measurements is the data sometimes referred to as the CO/CA Latino data. While we have been able to include these data in our database, there are some properties that remain opaque because of the multiple files we received, each of which only contains partial data. For instance, in one excel file, the same participant ID numbers appear in both the California *and* the Colorado data. Of course, this doesn't mean that all of these individuals moved. Ordinarily, I would give them different ID numbers, but the individuals for whom this is true are missing most of their essays from the Colorado data. This is just an example of some of the problems associated with these data. **As a solution** I would suggest that whoever is responsible for the curation of these data simply reorganize them entirely in a way that is more intuitive. Kevin Binning sent me the majority of these data, and while I appreciate his prompt responses, it's difficult to make heads or tails of what he has sent me and explained over email. It would be much simpler if he could deliver a single file (or a set of relational files!) that contain the information I've described here.

## Women & Math study
We are still missing the essays from this study. The last contact we had about this was with Krisitin Layous on 1/14/15. She indicated that the essays were still on hard copy and in Geoff's lab somewhere.

## Affirmation choice
This refers to data from Silverman, Logel, & Cohen (2013, *JESP*). We obtained scanned copies of the essays, but have no demographic information, outcome information, nor do we have subject ID numbers for the scanned essays. Arielle Silverman has suggested that it may not be possible to obtain this information. We have made repeated requests from Christine Logel, but have not yet received anything. The last time of contact was 5/7.

## Surgeons
These data (Salles, Cohen & Mueller, 2012 *Journal of the American College of Surgeons*) are largely not usable at the moment. The file we were sent is missing labels for the columns, so aside from some obvious ones (e.g. essays), it isn't clear which variable each column contains. Additionally, much of the data is missing and the essays are cut off in the middle of sentences.

## Prompts
We will eventually add in the prompt text for all additional studies included

## Values selected
We are just beginning to add a variable that will indicate which value the writer selected.

#Conclusions
This database provides a foundation on which a wide variety of analyses can be built. It is straightforward to use, simple to extend, and can be easily shared. While the data are already largely de-identified, they can be completely de-identified by simply not sharing the demographics file. 

For further readings on the format and structure of relational databases such as this one, or for the process of [data wrangling](https://en.wikipedia.org/wiki/Data_wrangling) required to turn unruly data into something neat, refer to the following sources:

Gentzkow and Shapiro (2014) [Code and Data for the Social Sciences: A Practitioner's Guide.](http://web.stanford.edu/~gentzkow/research/CodeAndData.pdf)  
Lohr. (2014) [For Big-Data Scientists, "Janitor Work" Is Key Hurdle to Insights.](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html) New York Times  
Wickham (2014) [Tidy data.](http://vita.had.co.nz/papers/tidy-data.pdf) Journal of Statistical Software.
