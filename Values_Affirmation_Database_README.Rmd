---
title: "Values Affirmation Database README"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  word_document: default
date: '`r format(Sys.time(), "%d %B, %Y")`'
---
----------------- 

#Introduction  

This document describes the database compiled as part of the NSF and ROADS sponsored research for the investigation of the values affirmation essays using natural language processing methodology.

The database consists of four files:

1. **demog** - This file features the demographic characteristics of the participants in the studies. As indicated by the number string in the filename, it was last updated on 9/11/2015. Similar dates are appended to the other files.
2. **essays** - This file includes the essays and their corresponding attributes.
3. **grades** - This file includes grades and other academic outcome measures for participants in the studies
4. **prompts** - This file includes characteristics of the prompts for each of the possible interventions.

Each file's name indicates the content of the data, plus the date on which the file was generated. At the time of this writing, the files have the names `demog9.11.15.csv`, `essays9.30.15.csv`, `grades10.5.15.csv`, and `prompts10.8.15.csv`.

#File format and structure
In order to make these files usable across a variety of software types and system environments, they have been stored as basic csv files. However, using commas to separate the field values in the essays and prompts file will obviously lead to problems, because the content of the fields occasionally contain columns. For that reason, these files are stored as pipe-separated values (a pipe is a |). It's possible to see the difference in structure by opening the files using a basic text-editor (textEdit on mac, notepad, sublimetext, textwrangler, etc)

The format of each file follows the general principles of [relational databases](https://en.wikipedia.org/wiki/Relational_model). Each file is stored in what is often referred to as a 'long' format, and each file features a specific type of observational unit (i.e. essays, participants, outcomes, or prompts). Within each file, these observational units are organized by the variables, which form columns, and the observations, which form rows. 

The variables for each file are listed below.

##Essays
```{r, echo=FALSE}
df.essays <- read.csv('../Data/3 CSV Files/essays9.30.15.csv', sep='|', quote="")
df.essays <- df.essays[which(df.essays$Essay != ''),]
names(df.essays)
```
##Participants
```{r, echo=F}
df.demographics <- read.csv('../Data/3 CSV Files/demog9.11.15.csv')
names(df.demographics)[-6:-8]
```
##Outcomes
```{r, echo=F}
df.outcomes <- read.csv('../Data/3 CSV Files/grades10.5.15.csv')
df.outcomes <- df.outcomes[which(df.outcomes$Grade != ''),]
df.outcomes <- df.outcomes[,-1]
names(df.outcomes)[-1]
```
##Prompts
```{r, echo=F, message=FALSE}
df.prompts <- read.csv('../Data/3 CSV Files/prompts10.8.15.csv', sep='|', quote="")
df.prompts <- df.prompts[,-1]
names(df.prompts)
```

Note that this file structure makes it very clear what information is contained where. Very little explication of the content of each file is needed here.

All files with the exception of the prompts share a common *key* of the participant ID number. This allows the analyst to join together data from each file using basic join operations. Where studies had duplicate ID numbers, we appended a `.x` to the end of the ID, where x is an integer, incremented each time we encountered a file that had ID numbers that duplicated ID numbers already incorporated into the database.

The prompts file, since the unit of measure is the intervention and not the individual, does not have a specific participant ID associated with each row. Instead, the information from this file can be joined to the others using one of the multiple keys it has in common with the other files. For instance, if the analyst were interested in joining the prompts to the Essays table, one could join by the combination of `Intervention_number`, `Intervention_date`, and `Condition`, which would correctly join the information in the two, yielding a new table that contained the prompt information for each essay oin the Essays file.

Furthermore, these data are stored in what is sometimes referred to as a 'long' format. This means that there are a minimal number of columns, and each table features measurements on a single element (i.e. essays, participants, outcomes, or prompts). The rows are the individual observations


#File contents
I now use these files to illustrate some basic properties about the database. We will investigate these files one-by-one. I will also go in depth for the Windsor data in some areas.

##Essays

- **Total Number of essays**
```{r, echo=F, message=FALSE}
length(df.essays$Essay)
```

- **Number of essays per study**
```{r, echo=F, message=FALSE}
table(df.essays$Study)
```

- **Number of essays per Cohort - Windsor data only**
```{r, echo=F, message=FALSE}
library(dplyr)
temp <- filter(df.essays, Study == 'Connecticut')
temp$Cohort <- droplevels(temp$Cohort)
table(temp$Cohort)
```

- **Number of interventions per Cohort - Windsor data only**
```{r, echo=F, message=FALSE}
table(temp$Intervention_number, temp$Cohort)
```

- **Intervention Dates per Cohort - Windsor data only**
```{r, echo=F, message=FALSE}
temp$Intervention_Date <- droplevels(temp$Intervention_Date)
temp$Intervention_Date <- as.Date(as.character(temp$Intervention_Date), format="%m/%d/%Y")
temp <- arrange(temp, Intervention_Date)
table(temp$Intervention_Date, temp$Cohort)
```

##Demographics

- **Number of participants**
```{r, echo=F, message=FALSE}
length(df.demographics$ID)
```

- **Number of participants per study**
```{r, echo=F, message=FALSE}
table(df.demographics$Study)
```

- **Breakdown of participant race by study**
```{r, echo=F, message=FALSE}
df.demographics$Ethnicity[which(df.demographics$Ethnicity == '#N/A')] <- NA
df.demographics$Ethnicity[which(df.demographics$Ethnicity == '')] <- NA
df.demographics$Ethnicity[which(df.demographics$Ethnicity == 'Other')] <- 'Other/Mixed'
df.demographics$Ethnicity <- droplevels(df.demographics$Ethnicity)
table(df.demographics$Ethnicity, df.demographics$Study)
```

- **Breakdown of participant gender by study**
```{r, echo=F, message=FALSE}
df.demographics$Gender[which(df.demographics$Gender == '#NULL!')] <- NA
df.demographics$Gender[which(df.demographics$Gender == '')] <- NA
df.demographics$Gender <- droplevels(df.demographics$Gender)
table(df.demographics$Study, df.demographics$Gender)
```

##Outcomes

- **Total number of outcome observations**
```{r, echo=F, message=FALSE}
length(df.outcomes$ID)
```

- **Types of outcomes per study**
```{r, echo=F, message=FALSE}
df.outcomes$Study <- droplevels(df.outcomes$Study)
table(df.outcomes$Grade_type, df.outcomes$Study)
```

- **Outcomes by date - Windsor data only**
```{r, echo=F, message=FALSE}
temp <- filter(df.outcomes, Study == 'Connecticut')
temp$est_Grade_date <- droplevels(temp$est_Grade_date)
levels(temp$est_Grade_date)[17:22] <- c('2/1/04', '2/1/05', '2/1/06', '6/30/04',
                                        '6/30/05', '6/30/06')
temp$est_Grade_date <- as.Date(as.character(temp$est_Grade_date), format="%m/%d/%y")
temp <- arrange(temp, est_Grade_date)
temp$Grade_type <- droplevels(temp$Grade_type)
table(temp$est_Grade_date, temp$Grade_type)
```

##Prompts
This file is still being created. The windsor data is finished, so we will limit our description to these data

- **Number of different prompts**
```{r, echo=F, message=FALSE}
temp <- filter(df.prompts, Study == 'Connecticut')
temp <- temp[which(temp$Essay.Prompt!=''),]
temp$Values <- droplevels(temp$Values)
length(unique(temp$Values))
```

- **Number of unique values combinations**
```{r, echo=F, message=FALSE}
temp <- filter(df.prompts, Study == 'Connecticut')
temp <- temp[which(temp$Essay.Prompt!=''),]
temp$Essay.Prompt <- droplevels(temp$Essay.Prompt)
length(unique(temp$Essay.Prompt))
```

- **Prompt text per Cohort**
```{r, echo=F, message=FALSE}
temp2 <- temp %>%
  group_by(Cohort, Intervention_number) %>%
  distinct(Essay.Prompt) %>%
  select(Intervention_number, Condition, Intervention_Date, Study, Cohort, Essay.Prompt)
```

```{r, echo=F, message=FALSE, results='asis'}
  writeLines(paste('###', temp2$Cohort[1]))
  #writeLines('####################\n')
  writeLines(paste('#####', 'Intervention Number ', temp2$Intervention_number[1]))
  #writeLines('^^^^^^^^^^^^^^^^^^\n')
  writeLines(paste('######','Condition', temp2$Condition[1]))
  #writeLines('~~~~~~~~~~~~~~~~~~~\n')
  writeLines(strwrap(temp2$Essay.Prompt[1]))
  writeLines('\n\n')
for(i in 2:length(temp2$Essay.Prompt)){
  if(temp2$Cohort[i] != temp2$Cohort[i-1]){
    writeLines(paste('###', temp2$Cohort[i], '  '))
    #writeLines('####################\n')
  }
  if(temp2$Intervention_number[i] != temp2$Intervention_number[i-1]){
    writeLines('\n-----\n')
    writeLines(paste('####', 'Intervention Number ', temp2$Intervention_number[i], '  '))
  }
  writeLines(paste('######', 'Condition', temp2$Condition[i], '  '))
  
  writeLines(strwrap(temp2$Essay.Prompt[i]))
  writeLines('\n')
}

```

#Remaining tasks and Residual concerns
## CO/CA Latino Study
The other dataset for which we have extensive longitudinal measurements is the data sometimes referred to as the CO/CA Latino data. While we have been able to include these data in our database, there are some properties that remain opaque because of the multiple files we received, each of which only contains partial data. For instance, in one excel file, the same participant ID numbers appear in both the California *and* the Colorado data. Of course, this doesn't mean that all of these individuals moved. Ordinarily, I would give them different ID numbers, but the individuals for whom this is true are missing most of their essays from the Colorado data. This is just an example of some of the problems associated with these data. **As a solution** I would suggest that whoever is responsible for the curation of these data simply reorganize them entirely in a way that is more intuitive. Kevin Binning sent me the majority of these data, and while I appreciate his prompt responses, it's difficult to make heads or tails of what he has sent me and explained over email. It would be much simpler if he could deliver a single file (or a set of relational files!) that contain the information I've described here.

## Women & Math study
We are still missing the essays from this study. The last contact we had about this was with Krisitin Layous on 1/14/15. She indicated that the essays were still on hard copy and in Geoff's lab somewhere.

## Affirmation choice
This refers to data from Silverman, Logel, & Cohen (2013, *JESP*). We obtained scanned copies of the essays, but have no demographic information, outcome information, nor do we have subject ID numbers for the scanned essays. Arielle Silverman has suggested that it may not be possible to obtain this information. We have made repeated requests from Christine Logel, but have not yet received anything. The last time of contact was 5/7.

## Surgeons
These data (Salles, Cohen & Mueller, 2012 *Journal of the American College of Surgeons*) are largely not usable at the moment. The file we were sent is missing labels for the columns, so aside from some obvious ones (e.g. essays), it isn't clear which variable each column contains. Additionally, much of the data is missing and the essays are cut off in the middle of sentences.

## Prompts
We have finished including prompt information for the Windsor data and are currently in the process of appending the same information for the other samples.

## Values selected
We are just beginning to add a variable that will indicate which value the writer selected.

## Annotated & Corrected essays
We will also soon add a second version of each essay that has been annotated for correct spelling and grammar. In order to best preserve the integrity of what the writer wanted to communicate, we used a limited approach, only annotating for additions (marked with `$$`), deletions (marked with `^^`), and substitutions (marked with `@@`) of words to create more grammatically well-formed sentences. Here is an example of an annotated essay:

**Original:**  
`This morning I had no breakfast and I got to school by car.  `

**Annotated:**  
`This morning $,$ I had $did not have$ no ^no^ breakfast $,$ and I got to school by car.  `

You can see that the annotated sentence not only introduces changes to create a more grammatically well-formed sentence, but *also* allows us to investigate the types of changes made.

#Conclusions
This database provides a foundation on which a wide variety of analyses can be built. It is straightforward to use, simple to extend, and can be easily shared. While the data are already largely de-identified, they can be completely de-identified by simply not sharing the demographics file. 

For further readings on the format and structure of relational databases such as this one, or for the process of [data wrangling](https://en.wikipedia.org/wiki/Data_wrangling) required to turn unruly data into something neat, refer to the following sources:

Gentzkow and Shapiro (2014) [Code and Data for the Social Sciences: A Practitioner's Guide.](http://web.stanford.edu/~gentzkow/research/CodeAndData.pdf)  
Lohr. (2014) [For Big-Data Scientists, "Janitor Work" Is Key Hurdle to Insights.](http://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html) New York Times  
Wickham (2014) [Tidy data.](http://vita.had.co.nz/papers/tidy-data.pdf) Journal of Statistical Software.
